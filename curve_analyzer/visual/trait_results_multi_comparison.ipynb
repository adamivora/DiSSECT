{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curve test analysis\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import json\n",
    "import multiprocessing\n",
    "from copy import deepcopy\n",
    "from time import sleep\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Markdown, Latex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = Path('../data')   # this comes handy for migrations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing functions\n",
    "### Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_results_file(file_path):\n",
    "    df_data, df_index = [], []\n",
    "\n",
    "    with open(file_path, 'r') as rf:\n",
    "        data = json.load(rf)\n",
    "        for curve_name, curve_data in data.items():\n",
    "            for params, results in curve_data.items():\n",
    "                dictionary = deepcopy(results)\n",
    "                dictionary.update(ast.literal_eval(params))\n",
    "                df_data.append(dictionary)\n",
    "                df_index.append(curve_name)\n",
    "    return df_index, df_data\n",
    "\n",
    "def plain_numerical2df(df_index, df_data, drop_cols=()):\n",
    "    columns = set(df_data[0].keys()).difference(drop_cols)\n",
    "    df_ = pd.DataFrame(df_data, index=df_index, columns=columns).fillna(0).astype(int)\n",
    "    df_['name'] = df_.index\n",
    "    df_['sim'] = df_.name.str.contains('sim').astype(int)\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ks_2samp\n",
    "\n",
    "def plot_df(df_, drop_cols=()):  # args cannot be mutable -- [] would cause problems\n",
    "    \"\"\"Logy histogram and relative density \n",
    "    => different sizes of sim groups can be shown together\n",
    "    \"\"\"\n",
    "    cols = df_.columns.drop(['name', 'sim',] + list(drop_cols))\n",
    "    for col in cols:\n",
    "        ax = df_.groupby('sim')[col].plot.hist(bins=100, logx=False, logy=False, figsize=(14, 6), \n",
    "                                               density=False, alpha=0.42, legend=True, xlim=(0, df[col].max()),)\n",
    "        ax2 = df_.groupby('sim')[col].plot.density(figsize=(14, 6), alpha=1.0, legend=False, logy=False,\n",
    "                                                   logx=False, xlim=(1, df[col].max()), ax=ax[0].twinx())\n",
    "        plt.title(col, fontsize='xx-large')\n",
    "        ax[0].legend(title='sim')\n",
    "        plt.show()\n",
    "        \n",
    "        from scipy.stats import ks_2samp\n",
    "\n",
    "def kl_divergence(orig_p, orig_q, epsilon=1e-5):\n",
    "    p, q = get_bins(orig_p), get_bins(orig_q)\n",
    "    return np.sum(np.where(p != 0, p * np.log(p / (q + epsilon)), 0))\n",
    "\n",
    "def get_bins(ser):\n",
    "    hist = np.histogram(ser,\n",
    "                        density=True,\n",
    "                        bins=50,\n",
    "                        range=(0, 5),\n",
    "                       )\n",
    "    return hist[0]\n",
    "\n",
    "def per_group(drop_cols=()):\n",
    "    def per_group_inner(df_):\n",
    "        res_ = {}\n",
    "        for on_col in df_.columns.drop(['name', 'sim',] + list(drop_cols)):\n",
    "            res_[(on_col, 'ks_stat', )] = ks_2samp(df_.loc[df_.sim == 0, on_col],\n",
    "                                                df_.loc[df_.sim == 1, on_col])[0]  # we need only the first value\n",
    "            res_[(on_col, 'kl_stat', )] = kl_divergence(df_.loc[df_.sim == 0, on_col],\n",
    "                                                       df_.loc[df_.sim == 1, on_col])\n",
    "        columns=pd.MultiIndex.from_tuples(res_.keys(), names=['col', 'stat'])\n",
    "        return pd.Series(res_, index=columns)\n",
    "    return per_group_inner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning understanding\n",
    "...it's so simple to code, you have to give it a try!\n",
    "I would test it again, once there are many test results per curve => the random forest / KMeans could find something interesting (== they would produce reasonable results => we can investigate those results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  # doctest: +SKIP\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import svm\n",
    "import sklearn\n",
    "from sklearn import ensemble\n",
    "\n",
    "def eval_classifier(classifier, X, y, ax):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    plot_confusion_matrix(classifier, X_test, y_test, ax=ax)  # doctest: +SKIP\n",
    "    \n",
    "def eval_classifiers(df_, drop_cols=()):\n",
    "    \"\"\"Check performance of different classifiers on test set (last 20 %)\"\"\"\n",
    "    df_ = df.sample(frac=1, random_state=0)  # should we \n",
    "    X = df_.drop(columns=['sim', 'name', ] + list(drop_cols))\n",
    "    y = df_.sim\n",
    "    \n",
    "    classifiers = [   # The hyperparams could be tuned/autotuned\n",
    "        SVC(random_state=0, degree=3),\n",
    "        SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=100),\n",
    "        sklearn.neighbors.KNeighborsClassifier(n_neighbors=2),\n",
    "        ensemble.RandomForestClassifier(10),\n",
    "    ]\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(15, 10))   # nrows * ncols = len(classifiers)\n",
    "\n",
    "    for cls, ax in zip(classifiers, axes.flatten()):\n",
    "        eval_classifier(cls, X, y, ax=ax)\n",
    "        ax.title.set_text(type(cls).__name__)\n",
    "        ax.title.set_fontsize('xx-large')\n",
    "    \n",
    "    plt.tight_layout()  \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A05\n",
    "> It would be great to have a real description here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(data_folder.glob('*.json')))   # list all files -> could be used to auto parse all files\n",
    "\n",
    "drop_cols = ('l', )  # comma needed for tuples\n",
    "df = plain_numerical2df(*parse_results_file(data_folder / 'a05_filtered.json'))  # *does tuple unpacking\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.describe())\n",
    "display(df.groupby('sim').describe().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the 'l' col needs to be skipped as it contains only 1 value \n",
    "# and causes LinAlg error when calculating the density\n",
    "plot_df(df.loc[df.l == 7], drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df(df.loc[df.l == 5], drop_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('l').apply(per_group(drop_cols))  # any groupby producing non-zero groups is supported "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_classifiers(df, drop_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ML classification is not good for any of the algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a25\n",
    "> It would be great to have a real description here\n",
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ('trace_factorization', )  # comma needed for tuples\n",
    "df_index, df_data = parse_results_file(data_folder / 'a25.json')\n",
    "df = plain_numerical2df(df_index, df_data, drop_cols)  # *does tuple unpacking\n",
    "df['trace_factorization'] = pd.Series([np.array(x['trace_factorization'])[:,0]   # ignore ones\n",
    "                                       for x in df_data], index=df_index)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate reasonable features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_min_ratio(x):\n",
    "    return np.max(x) / np.min(x)\n",
    "\n",
    "feature_fns = np.min, np.max, np.mean, np.median, max_min_ratio\n",
    "\n",
    "for fn in feature_fns:\n",
    "    df[f'log10({fn.__qualname__})'] = np.log10(df.trace_factorization.apply(fn).astype(float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown('### Numerical comparison'))\n",
    "display(df.describe())\n",
    "display(df.groupby('sim').describe().T)\n",
    "\n",
    "display(Markdown('### Visual comparison'))\n",
    "plot_df(df, drop_cols)   # subset it or something\n",
    "\n",
    "display(Markdown('### Metric comparison'))\n",
    "display(df.groupby(pd.Series(True, index=df.index)).apply(per_group(drop_cols)))   # consistency with groupby\n",
    "# display(per_group(['trace_factorization'])(df).to_frame().T)   # those 2 lines are equivalent\n",
    "\n",
    "display(Markdown('### ML comparison'))\n",
    "eval_classifiers(df, drop_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those are some reasonable results for basic ML classifiers with un-tuned hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
